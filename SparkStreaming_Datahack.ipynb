{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkStreaming_Datahack",
      "provenance": [],
      "authorship_tag": "ABX9TyMvh7pOxprfdkWPxiNDpvol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masfworld/sparkstreaming_datahack_webinar/blob/master/SparkStreaming_Datahack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft7SwygNjtKP",
        "colab_type": "text"
      },
      "source": [
        "## Spark Streaming\n",
        "\n",
        "\n",
        "<a href=\"https://datahack.es\">\n",
        "  <img src=\"https://www.datahack.es/wp-content/uploads/2019/10/Todo_vertical.png\" Big Data Spain\"\" width=\"40%\">\n",
        "  \n",
        "  \n",
        "**Author**:\n",
        "miguel.sotomayor@sidesna.es\\\n",
        "https://www.linkedin.com/in/miguelsotomayorf/ \\\n",
        "https://github.com/masfworld\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9d0x1OTh2N",
        "colab_type": "text"
      },
      "source": [
        "# Prerrequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbGSM3_NM-z",
        "colab_type": "code",
        "outputId": "21a75ce9-81d0-4fa0-e3cd-63275c0f94f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install py4j"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py4j\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 22.2MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 71kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: py4j\n",
            "Successfully installed py4j-0.10.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_HtvSAj4sI",
        "colab_type": "code",
        "outputId": "35b75b34-b2de-4887-f348-e45193e24677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/libs/libs-kafka.zip --directory-prefix=/content/spark-2.4.5-bin-hadoop2.7/jars/\n",
        "!unzip -n /content/spark-2.4.5-bin-hadoop2.7/jars/libs-kafka.zip -d /content/spark-2.4.5-bin-hadoop2.7/jars/\n",
        "!ls /content/spark-2.4.5-bin-hadoop2.7/jars/*kafka*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/spark-2.4.5-bin-hadoop2.7/jars/libs-kafka.zip\n",
            "  inflating: /content/spark-2.4.5-bin-hadoop2.7/jars/kafka-clients-2.0.0.jar  \n",
            "  inflating: /content/spark-2.4.5-bin-hadoop2.7/jars/lz4-java-1.4.1.jar  \n",
            "  inflating: /content/spark-2.4.5-bin-hadoop2.7/jars/slf4j-api-1.7.25.jar  \n",
            "  inflating: /content/spark-2.4.5-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar  \n",
            "  inflating: /content/spark-2.4.5-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.11-2.4.5.jar  \n",
            "  inflating: /content/spark-2.4.5-bin-hadoop2.7/jars/unused-1.0.0.jar  \n",
            "/content/spark-2.4.5-bin-hadoop2.7/jars/kafka-clients-2.0.0.jar\n",
            "/content/spark-2.4.5-bin-hadoop2.7/jars/libs-kafka.zip\n",
            "/content/spark-2.4.5-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.11-2.4.5.jar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0qDLAxMTUYQ",
        "colab_type": "text"
      },
      "source": [
        "Define the environment (Java & Spark homes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSd4dfANNndH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdFv-xxITa2J",
        "colab_type": "text"
      },
      "source": [
        "Starting Spark Session and print the version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLMbVBATf9K",
        "colab_type": "code",
        "outputId": "6ee87eb2-d32e-4a76-f9b5-f3018fb7b78b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import findspark\n",
        "findspark.add_packages([\"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\"])\n",
        "findspark.add_jars([\"/content/spark-2.4.5-bin-hadoop2.7/jars/kafka-clients-2.0.0.jar\",\"/content/spark-2.4.5-bin-hadoop2.7/jars/lz4-java-1.4.1-jar\",\"/content/spark-2.4.5-bin-hadoop2.7/jars/scala-library-2.11.12.jar\",\"/content/spark-2.4.5-bin-hadoop2.7/jars/slf4j-api-1.7.25.jar\",\"/content/spark-2.4.5-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar\",\"/content/spark-2.4.5-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.11-2.4.5.jar\",\"/content/spark-2.4.5-bin-hadoop2.7/jars/spark-tags_2.11-2.4.5.jar\",\"/content/spark-2.4.5-bin-hadoop2.7/jars/unused-1.0.0.jar\"])\n",
        "findspark.init(\"spark-2.4.5-bin-hadoop2.7\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLtpaEJbTkG0",
        "colab_type": "text"
      },
      "source": [
        "# Streaming from a directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WNBQqPGOOFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head /content/sample_data/california_housing_train.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_j_AKjkQhwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p /content/california_housing\n",
        "!cp /content/sample_data/california_housing_train.csv /content/california_housing/\n",
        "!ls /content/california_housing/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXSAqezrOlt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import StructType\n",
        "\n",
        "# Read all the csv files written atomically in a directory\n",
        "housingSchema = StructType()\\\n",
        ".add(\"longitude\", \"double\")\\\n",
        ".add(\"latitude\", \"double\")\\\n",
        ".add(\"housing_median_age\", \"double\")\\\n",
        ".add(\"total_rooms\", \"double\")\\\n",
        ".add(\"total_bedrooms\", \"double\")\\\n",
        ".add(\"population\", \"double\")\\\n",
        ".add(\"households\", \"double\")\\\n",
        ".add(\"median_income\", \"double\")\\\n",
        ".add(\"median_house_value\", \"double\")\n",
        "\n",
        "housing_df = spark \\\n",
        "    .readStream \\\n",
        "    .option(\"sep\", \",\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(housingSchema) \\\n",
        "    .csv(\"/content/california_housing/\")  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IEOYT12akDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "housing_avg = housing_df.agg(avg(col('total_bedrooms')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDMK03vYRmwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "housing_avg \\\n",
        "  .writeStream \\\n",
        "  .outputMode(\"complete\") \\\n",
        "  .format(\"memory\") \\\n",
        "  .queryName(\"housing_avg\") \\\n",
        "  .start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsRozpZJaOjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.sql(\"select * from housing_avg\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l516Yl0Vdage",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/sample_data/california_housing_test.csv /content/california_housing/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FSWNLHuhBsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.sql(\"select * from housing_avg\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPqf9q2HjON4",
        "colab_type": "text"
      },
      "source": [
        "# Streaming From Kafka"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXLEwaLGjUUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kafka_df = spark \\\n",
        "  .readStream \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
        "  .option(\"subscribe\", \"topic1\") \\\n",
        "  .load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzxwPSrujfCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}